---
id: rel-2026-02-11-ai-backend-target-architecture
version: v0.47.0
title: "AI backend target architecture draft"
date: 2026-02-11
published_at: 2026-02-11T06:55:46Z
status: draft
notify_in_app: false
in_app_hours: 24
summary: "Implemented the first operational benchmark stack with persisted sessions/runs, export/cleanup endpoints, and an admin benchmark workspace."
---

## Changes
- [ ] [Internal] ğŸ§± Added a concrete target architecture doc for provider-agnostic trip generation, including adapter interfaces, endpoint contracts, and rollout phases.
- [ ] [Internal] ğŸ§ª Defined an internal benchmark workspace design with multi-model test execution, persistent sessions/runs, and shareable result URLs.
- [ ] [Internal] ğŸ§¹ Documented benchmark trip tagging plus bulk cleanup flows to archive or purge test-generated trip data safely.
- [ ] [Internal] ğŸ” Added a dedicated auth/roles migration note for replacing temporary static admin headers during login/register rollout.
- [ ] [Internal] ğŸ§­ Locked v1 scope decisions for `/admin/ai-benchmark`, Classic-only benchmark inputs, and simulated-login-gated provider selector visibility in `/create-trip`.
- [ ] [Internal] ğŸ§ª Implemented `/admin/ai-benchmark` as an operational workspace with classic benchmark inputs, dynamic model rows, test-all, per-row rerun, persisted table reload, and download/cleanup actions.
- [ ] [Internal] ğŸ—‚ï¸ Added a curated AI model catalog for Gemini/OpenAI/Anthropic with preferred/current runtime metadata and estimated per-query cost labels.
- [ ] [Internal] ğŸ›ï¸ Added a simulated-login-only internal model selector to the classic create-trip flow, grouped by provider and wired to generation options.
- [ ] [Internal] ğŸ§¾ Stored AI generation metadata on trips and surfaced provider/model/timestamp in the trip information modal.
- [ ] [Internal] ğŸŒ Added `/api/ai/generate` edge endpoint for server-side itinerary generation and switched classic generation to call server first with local Gemini fallback.
- [ ] [Internal] ğŸ”‘ Added provider/admin environment-key scaffolding (`GEMINI_API_KEY`, `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `OPENROUTER_API_KEY`, `TF_ADMIN_API_KEY`) in env docs and Netlify config.
- [ ] [Internal] ğŸ›‚ Implemented `/api/internal/ai/benchmark` execution flow with temporary `x-tf-admin-key` guard plus Supabase bearer auth, real model calls, run persistence, and benchmark trip creation/flagging.
- [ ] [Internal] ğŸ—ƒï¸ Added benchmark persistence schema (`ai_benchmark_sessions`, `ai_benchmark_runs`) with indexes, updated_at trigger, and owner-scoped RLS policies to `docs/supabase.sql`.
- [ ] [Internal] ğŸ”Œ Enabled OpenAI and Anthropic execution paths in `/api/ai/generate` with allowlisted models and server-key checks; OpenRouter remains intentionally disabled for now.
- [ ] [Internal] ğŸ“¦ Implemented benchmark export endpoints (`run` JSON and `session` ZIP with one file per run) and cleanup endpoint for bulk benchmark trip/session removal.
- [ ] [Internal] ğŸ§© Added reusable classic prompt builder export and model-catalog release sorting to keep benchmark prompts aligned with create-trip behavior and prioritize newer models in selectors.
- [ ] [Internal] ğŸ¨ Replaced the create-trip internal AI model override dropdown with a styled shadcn/Radix select component to match UI standards.
- [ ] [Internal] ğŸ¨ Upgraded `/admin/ai-benchmark` model/budget/pace selects to styled shadcn/Radix selects with clearer model metadata and preferred/runtime badges.
- [ ] [Internal] ğŸ§­ Documented local testing requirement for internal edge benchmark routes via `npx netlify dev` (`localhost:8888`) instead of Vite-only dev routing.
- [ ] [Internal] âš¡ Added optimistic benchmark table feedback (pre-filled running rows + live latency updates), persisted model-row selections with sensible defaults, and streamlined benchmark action labels/layout.
- [ ] [Internal] ğŸ§· Changed benchmark trip persistence path to direct `trips` insert for `source_kind='ai_benchmark'` so benchmark runs are not blocked by the normal RPC trip-limit guard.
- [ ] [Internal] âœ… Enforced benchmark output validation gate before trip persistence so structurally-invalid model outputs fail as runs instead of creating empty/low-quality benchmark trips.
- [ ] [Internal] ğŸ¯ Added persisted run ranking (`good`/`medium`/`bad`) with new internal rating endpoint and Supabase schema fields on `ai_benchmark_runs`.
- [ ] [Internal] ğŸ“Š Added benchmark table controls for provider filtering, failed/unrated toggles, sortable run timestamp, and a model dashboard with average latency/cost/satisfaction (plus vote/run counts).
- [ ] [Internal] ğŸ§ª Added compact error rendering with JSON-detail modal and syntax-highlighted payload view for failed benchmark runs.
- [ ] [Internal] ğŸ¨ Refined model select option readability with right-aligned badges, cost metadata in options, and short selected labels in triggers (benchmark + create-trip internal selector).
- [ ] [Internal] ğŸ§¾ Added a benchmark prompt preview action to generate, inspect, copy, and download the exact full prompt built from current classic benchmark form settings.
- [ ] [Internal] ğŸ“¦ Extended session ZIP export with optional log bundle support (`includeLogs=1`) to include scenario/prompt artifacts plus run-level diagnostics for prompt iteration.
- [ ] [Internal] ğŸ¨ Standardized AI-generated trip color defaults to the classic palette with explicit palette metadata so new benchmark/runtime trips no longer default to low-contrast pastel map colors.
- [ ] [Internal] ğŸ§¼ Removed legacy benchmark palette migration controls from admin/runtime flow; benchmark color consistency now relies on corrected defaults for newly generated benchmark trips.
- [ ] [Internal] ğŸ§° Improved benchmark error diagnostics by preserving parseable provider error payloads on failed runs and recursively decoding nested/stringified JSON in the error modal.
- [ ] [Internal] ğŸ§­ Added shared transport-mode contract + normalization layer (`shared/transportModes.ts`) and linked update workflow docs to keep enum, aliases, prompt guidance, and UI behavior in sync.
- [ ] [Internal] ğŸš¦ Tightened travel prompt/output contract with explicit canonical transport-mode and duration format rules, including positive/negative examples for faster prompt iteration.
- [ ] [Internal] ğŸ§® Added shared flexible duration parsers (`shared/durationParsing.ts`) to normalize textual hour/minute values into canonical numeric durations during trip building.
- [ ] [Internal] âœ… Expanded benchmark validation checks for top-level contract, required fields, transport mode formatting, duration formatting, and country info completeness with detailed per-run check modal in admin UI.
- [ ] [Internal] ğŸ¯ Normalized unknown transport modes to `na`, suppressed transport map icons for unset modes, and added dashed unset styling for transportation panels/chips to highlight missing mode selection.
- [ ] [Internal] ğŸ§± Renamed the canonical runtime generation module to `services/aiService.ts` and kept `services/geminiService.ts` as a backward-compatible re-export shim for staged migration.
- [ ] [Internal] ğŸš¨ Split benchmark validation outcomes into blocking errors vs non-blocking warnings, persisted warning details in run validation payloads, and added warning visibility/filtering in `/admin/ai-benchmark`.
- [ ] [Internal] â±ï¸ Switched benchmark execution to async edge background processing (`waitUntil`) with admin-page polling so deployed benchmark runs no longer time out when model generations are slow.
- [ ] [Internal] ğŸ”„ Added benchmark-page startup bootstrap to auto-load the latest persisted benchmark session when no `session` URL param is present, so refresh does not appear to lose prior runs.
- [ ] [Internal] ğŸ›‘ Added benchmark cancellation support (`POST /api/internal/ai/benchmark/cancel`) with per-run and per-session abort actions in `/admin/ai-benchmark`.
- [ ] [Internal] ğŸ“¡ Kept benchmark polling/live-latency updates active after reloading an in-progress session so running rows continue updating until completion or manual abort.
- [ ] [Internal] â³ Switched benchmark execution off nested `/api/ai/generate` calls to direct provider runtime execution with a dedicated benchmark timeout budget (`AI_BENCHMARK_PROVIDER_TIMEOUT_MS`, default 90s) to reduce edge timeout failures.
- [ ] [Internal] ğŸ§° Added provider timeout environment controls for runtime and benchmark paths (`AI_GENERATE_PROVIDER_TIMEOUT_MS`, `AI_BENCHMARK_PROVIDER_TIMEOUT_MS`) and documented expected defaults.
- [ ] [Internal] ğŸ›¡ï¸ Enforced a hard 90s minimum benchmark provider timeout so low env overrides (for example `10000`) can no longer force premature benchmark request aborts.
- [ ] [Internal] ğŸ§­ Improved `/admin/ai-benchmark` execution UX by auto-scrolling to results on `Test all`, removing redundant â€œLeft panelâ€ labeling, and tightening the internal auth card layout on small screens.
- [ ] [Internal] ğŸš¦ Increased benchmark parallel execution cap to 5 workers (with automatic queueing for additional selected models) and surfaced this directly in the model-selection UI.
- [ ] [Internal] ğŸ’µ Added benchmark cost-display fallback to model-catalog estimates when exact provider `cost_usd` is unavailable, plus clarifying copy in the results section.
- [ ] [Internal] ğŸ§ª Downgraded `countryInfo` benchmark validation failures to non-blocking warnings, tightened AI prompt/schema guidance to require numeric `exchangeRate`, and hardened destination info UI to disable currency conversion when malformed exchange data is returned.
- [ ] [Internal] ğŸ›°ï¸ Replaced the OpenRouter runtime stub with a real adapter call path (JSON extraction, usage/cost metadata capture, and transient 429/5xx retry handling) while keeping provider keys server-only.
- [ ] [Internal] ğŸ¤– Expanded the curated benchmark/runtime model catalog with newer Gemini/OpenAI/Anthropic entries plus curated OpenRouter free-model alternatives for broader comparison coverage.
- [ ] [Internal] ğŸ§ª Added regression tests for provider allowlist enforcement, OpenRouter runtime failure/retry behavior, and model-catalog default/sorting/grouping safeguards.
- [ ] [Internal] ğŸ“¡ Added persistent AI call telemetry (`ai_generation_events`) for runtime + benchmark execution, capturing provider/model/status/duration/token/cost/error metadata server-side.
- [ ] [Internal] ğŸ“Š Added `/api/internal/ai/benchmark/telemetry` plus new admin dashboard telemetry cards/charts/filter controls for time window, source, and provider breakdown.
- [ ] [Internal] ğŸ“ˆ Added Umami custom events for create-trip AI request success/failure and Gemini fallback outcomes to correlate UX and backend behavior.
- [ ] [Internal] ğŸ¤– Expanded Anthropic/OpenRouter model coverage with Claude Sonnet 4.6 plus curated OpenRouter additions (`GLM 5`, `DeepSeek V3.2`, `Grok 4.1 Fast`, `MiniMax M2.5`, `Kimi K2.5`).
- [ ] [Internal] ğŸšš Updated feature-branch Netlify deployment guidance to use `dotenv` local build + `netlify deploy --no-build` to prevent masked Supabase browser env values in auth testing.
- [ ] [Internal] ğŸ—„ï¸ Added owner-scoped benchmark preferences persistence (`ai_benchmark_preferences`) plus `/api/internal/ai/benchmark/preferences` so admin model targets and benchmark presets are stored in DB instead of browser local storage.
- [ ] [Internal] ğŸ§© Refactored `/admin/ai-benchmark` into a compact control surface with modal-based preset editing/creation and modal-based model target management using the global `AppModal` pattern.
- [ ] [Internal] ğŸ“ˆ Added a fixed 7-day telemetry snapshot at the top of `/admin/ai-benchmark` (Tremor cards + charts) and auto-refresh telemetry reload when benchmark runs finish.
- [ ] [Internal] ğŸ§ª Added normalization tests for benchmark preference payload/model-target/preset handling to cover default preset generation and invalid-payload recovery.
- [ ] [Internal] ğŸ§­ Added a dedicated `/admin/ai-benchmark/telemetry` workspace route with richer filters, ranking cards (speed/cost/value), charts, and recent-call diagnostics.
- [ ] [Internal] ğŸª¶ Slimmed `/admin/ai-benchmark` telemetry into a lightweight 7-day quick-view (top fastest/cheapest/best-value model cards) plus direct deep-link to the full telemetry workspace.
- [ ] [Internal] âœ… Changed telemetry model-ranking calculations to use successful calls only, so fastest/cheapest/value leaderboards no longer reward aborted/failed runs.
- [ ] [Internal] ğŸ” Added OpenAI runtime fallback from `v1/chat/completions` to `v1/responses` for non-chat model IDs to reduce `OPENAI_REQUEST_FAILED` errors on newer model entries.
- [ ] [Internal] ğŸ¨ Reworked the telemetry workspace with a compact filter strip, modern card wrappers, Tremor KPI cards, Tremor bar-list leaderboards, and deeper chart compositions for trend/provider/model analysis.
